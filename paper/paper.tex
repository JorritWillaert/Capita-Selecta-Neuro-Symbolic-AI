\documentclass[english]{sobraep}
\usepackage{minted}

\title{Visual Question Answering with DeepProbLog}

\author{Jorrit Willaert$^{1}$ \\
	\normalsize $^{1}$Catholic University of Leuven, Leuven -- Belgium \\
	\normalsize e-mail: jorrit.willaert@student.kuleuven.be
}

\begin{document}

\maketitle

% TODO fix the numbering (also subsection = A)
% TODO also maybe include some results from other papers.

\begin{abstract}
	TODO
\end{abstract}

\begin{keywords}
	Neuro Symbolic AI, Visual Question Answering, DeepProbLog, Problog, Convolutional Neural Networks
\end{keywords}

\section{INTRODUCTION}
The Neuro Symbolic AI field is interested in building a bridge between the robustness of probabilistic knowledge, with the well-known popularity and proven strengths of deep neural networks. DeepProbLog \cite{deepproblog} offers this ability, by using both the strengths of neural networks (i.e. system 1, typical subconscious tasks such as visual recognition, the processing of languages, \dots), along with the strengths of rule-based probabilistic systems (i.e. system 2, slow, sequential thinking such as the derivation of a proof). 

This paper elaborates on an application that requires both systems to be used, namely Visual Question Answering. System 1 will be required in order to gain an understanding of the image under investigation, with in particular their shapes and colors. System 2, on the other hand, will use this extracted information for deriving certain properties of objects\footnote{For example, finding the shape of the green object, or deriving if it is located on the left hand side of the image.}, or even for capturing the relations\footnote{Here, one could think of counting the number of circles in the image.} between the objects. 

\section{ORGANIZATION OF THE PAPER}
This paper will first provide some necessary background in Section \ref{sec:literature_survey} on certain types of datasets that are typically used for Visual Question Answering purposes. Then, Section \ref{sec:approach} will dive deeper in the different components constituting the overall system. Section \ref{sec:experiments}, furthermore, provides some results of conducted experiments. A main focus in this paper is to outline the advantages of using a Neuro-Symbolic AI approach (offered by the DeepProbLog framework), instead of a purely neural network based approach. These experiments are listed in Section \ref{sec:experiments}. Finally, the main findings of this paper are repeated in Section \ref{sec:conclusions}.

\section{LITERATURE SURVEY}
\label{sec:literature_survey}
The application focuses on Visual Question Answering (VQA), for which huge datasets are present, along with very sophisticated methods. The best known dataset for VQA is CLEVR \cite{clevr_dataset}, which contains 100k images with one million questions. An example image is given in Figure \ref{fig:sample_image_clevr}, while example questions are:
\begin{itemize}
    \item Are there an equal number of large things and metal spheres?
    \item What size is the cylinder that is left of the brown metal thing that is left of the big sphere?
    \item How many objects are either small cylinders or metal things?
\end{itemize}

\begin{figure}[htp]
    \begin{center}
    \includegraphics[width=0.3\textwidth]{clevr.jpg}
    \captionsetup{justification=centering}
    \caption{A sample image from the CLEVR dataset \cite{clevr_dataset}}
    \label{fig:sample_image_clevr}
    \end{center}
\end{figure}

Clearly, both system 1 and system 2 are actively used when answering these questions. One could wonder if neural networks alone could answer these questions without having an explicit system 2 encoding (i.e. the rule based knowledge base). Intuitively, it makes sense that if certain facts of the world are known\footnote{Facts can be encoded, e.g. counting the number of spheres is simply a matter of detecting all the spheres in the image, after which a mathematical summation is a statement in the knowledge base.}, learning can proceed much more quickly\footnote{Not to say that learning might even be impossible if a lot of background knowledge is required.}. Seen from an optimization viewpoint, errors made during prediction in this setup can be targeted exactly, which makes the optimization process more targeted as well, and hence more efficient. Finally, this paper also provides evidence for these statements, since in Subsection \ref{subsec:experiments_NN_vs_deepproblog}, the comparison between a VQA implementation with DeepProbLog is made with a purely CNN based approach. 

This paper is inspired on the CLEVR dataset, but uses however a much more simplified version. In essence, it is almost like the Sort-Of-CLEVR dataset \cite{sort_of_clevr_dataset}. This Sort-Of-CLEVR dataset contains images such as in Figure \ref{fig:sample_image_sort_of_clevr}, while asking questions such as:
\begin{itemize}
    \item Non-relational questions: the shape, horizontal or vertical location of an object.
    \item Relational questions: shape of the closest/furthest object to the object under investigation, or the number of objects with the same shape.
\end{itemize}

\begin{figure}[htp]
    \begin{center}
    \fbox{\includegraphics[width=0.25\textwidth]{sort_of_clevr.png}}
    \captionsetup{justification=centering}
    \caption{A sample image from the CLEVR dataset \cite{sort_of_clevr_dataset}}
    \label{fig:sample_image_sort_of_clevr}
    \end{center}
\end{figure}

As illustrated earlier, both system 1 and system 2 are required for these types of VQA's.

Finally, since this application uses DeepProbLog, quite some time was spent in digesting the DeepProbLog paper \cite{deepproblog}, along with understanding the  examples provided in the code repository \cite{deepproblog_code}.

\section{APPROACH}
\label{sec:approach}
The implementation process involved three main parts:
\begin{enumerate}
    \item Generation of data.
    \item Linking the data and controlling the training process in pure Python code.
    \item Creation of the logical part with DeepProbLog statements.
\end{enumerate}

\subsection{Generation of data}
As mentioned in Section \ref{sec:literature_survey}, the data used in this application is based on the Sort-Of-CLEVR dataset, with one extra simplification. Given that the logical part will have to decide whether an object is for example located on the left side of an image, the neural network will have to convey positional information to the logical part. Hence, each discrete position will have to be encoded by a possible outcome of the neural network. Therefore, objects may only be located at certain places in a grid. In this paper, results on a grid of 2x2 and 6x6 are discussed.

The data generator that was used for the creation of the Sort-Of-CLEVR dataset has been modified in order to place objects in the mentioned grid positions \cite{sort_of_clevr_dataset}. An example of a generated image is given in Figure \ref{fig:sample_image_own_dataset}, where the difference with Figure \ref{fig:sample_image_sort_of_clevr} is the grid-layout.

\begin{figure}[htp]
    \begin{center}
    \fbox{\includegraphics[width=0.3\textwidth]{sample_image_dataset.png}}
    \captionsetup{justification=centering}
    \caption{A sample image from the dataset that has been used for this application}
    \label{fig:sample_image_own_dataset}
    \end{center}
\end{figure}

Each specified color will have an object located somewhere in the grid, of which the shape can be a square or a circle.

These images are accompanied with a question about a random object, which can be one of the following:
\begin{itemize}
    \item Non-binary - What is the shape of this object \footnote{I.e. the shape will be either a square or a circle.}?
    \item Non-binary - Is this object located on the left side of the image?
    \item Non-binary - Is this object located on the bottom side of the image?
    \item Binary - How many objects have the same shape as this object?
\end{itemize}

These questions are encoded in a one-hot encoding, after which they are stored in a CSV file, along with the expected answers. To make the training process more efficient, a training and test dataset has been generated beforehand.

\subsection{Controlling the training process}
The overall training process is controlled via the Python API of DeepProbLog, along with general PyTorch implementations of the CNN's.
First of all, CNN's are defined with PyTorch. A relatively simple network is used, where the input is given as a square RGB image of 100 pixels wide, which is transformed by the CNN into 72 output features for the 6x6 grid\footnote{For the 2x2 grid example, 8 output features are required.}. Each color that is present in the image has its accompanied CNN network, hence the 72 output features encode the possible positions of the object with that color, along with their shape, which can be either square or circular ($6 \cdot 6 \cdot 2 = 72$).

The final thing (besides the logical rule encodings) required before commencing the training process, are the data loaders. The most challenging part here is the transformation from the generated data to specific query mappings and their outcome. 

\subsection{Logical rule encodings}
Once the CNN belonging to a specific color has determined the position and the shape of that object, logical rules can deduce whether this object is located on the left hand side of the image, on the bottom side, and how many objects have the same shape. The logical rule program has been listed in Appendix \ref{appendix:logical_rule_encodings}.

\section{EXPERIMENTS}
\label{sec:experiments}
The main focus of this paper is to outline the advantages of using a Neuro-Symbolic AI approach (offered by the DeepProbLog framework), instead of a purely neural network based approach. Therefore, another purely neural network based approach had to be implemented. Not all details will be listed here, but the main idea is that the image has to be fused with the question, after which a prediction can be made\footnote{Possible answers on the questions are: rectangle, circle, yes, no, 1, 2, 3, 4, 5 and 6. Hence, the neural network will have 10 output nodes.}. The general structure of this network is given in Figure \ref{fig:pure_nn_network}.

\begin{figure}[htp]
    \begin{center}
    \includegraphics[width=0.45\textwidth]{pure_nn_architecture.png} 
    \captionsetup{justification=centering}
    \caption{Abstract representation of the purely neural network based architecture}
    \label{fig:pure_nn_network}
    \end{center}
\end{figure}

\subsection{COMPARISONS WITH PURE SYSTEM 1 APPROACHES}
\label{subsec:experiments_NN_vs_deepproblog}
\subsubsection{Experiment: 2x2 grid}
The loss curves of both the DeepProbLog approach, as well as the purely neural network based approach, are visualized respectively in Figure \ref{fig:loss_curve_deepproblog_2x2} and Figure \ref{fig:loss_curve_pure_NN_2x2}.

\begin{figure}[htp]
    \begin{center}
    \includegraphics[width=0.45\textwidth]{loss_curve_deepproblog_2x2.png} 
    \captionsetup{justification=centering}
    \caption{DeepProbLog 2x2: Loss curve}
    \label{fig:loss_curve_deepproblog_2x2}
    \end{center}
\end{figure}

\begin{figure}[htp]
    \begin{center}
    \includegraphics[width=0.45\textwidth]{loss_curve_pure_NN_2x2.png} 
    \captionsetup{justification=centering}
    \caption{Pure NN based approach: Loss curve}
    \label{fig:loss_curve_pure_NN_2x2}
    \end{center}
\end{figure}
An extremely important remark to be made is the difference between `Number of iterations' and `Number of epochs'. By the number of iterations, the number of forward and backward passes of a batch (with size 32) is meant, whereas the number of epochs denote the number of times all the images of the training set are forward and backwardly passed. In this application, a training size of 10 000 was used, hence one epoch consists of 312.5 iterations.

From the loss curves, it is clear that both approaches seem to converge to an accuracy of 100\%. However, DeepProbLog only requires around 40 iterations, whereas the purely neural network based approach requires at least 250 000 iterations. This again demonstrates the value of neuro-symbolic AI.

On the other hand, one has to consider the actual running times for those iterations. DeepProbLog takes around 10 minutes to finish its 160 iterations, while the purely neural network based approach requires around 15 minutes to finish 250 000 iterations. Taking into consideration that the purely neural network based approach can be accelerated massively (by using GPU's), while DeepProbLog can't\footnote{DeepProbLog offers the ability to send the CNN to a GPU for faster inference, however, the arithmetic operators (i.e. semirings) of DeepProbLog work on the CPU. These arithmetic operators possess by far the highest computational cost.}, it is clear that DeepProbLog trains much more targeted, but is computationally extremely heavy (at least for now).

\subsubsection{Experiment: 6x6 grid}



The network based on pure neural predicates is able to recognize the questions quickly, however, seems to experience difficulties when having to decide for the correct answer. This can clearly be seen in the confusion matrix (Figure \ref{fig:confusion_matrix_6x6_pure_NN}). % TODO also talk about confusion matrix of deepproblog

\begin{figure}[htp]
    \begin{center}
    \includegraphics[width=0.3\textwidth]{confusion_matrix_pure_NN_6x6_clear_mistakes.png} 
    \captionsetup{justification=centering}
    \caption{The confusion matrix of the 6x6 dataset on a purely Neural Network based model}
    \label{fig:confusion_matrix_6x6_pure_NN}
    \end{center}
\end{figure}

% Denote difference between epochs and iterations!
% Trained on CPU

It is clear that the network trained with DeepProbLog is able to learn way faster then the purely NN based approach, if measured in the number of iterations \footnote{By the number of iterations, the number of training steps on a batch is meant.}. However, the differences in training time are much less significant% TODO give concrete measures
, due to the high computational cost associated with the arithmetic parts of the circuit (i.e. the semirings).

\section{CONCLUSIONS}
\label{sec:conclusions}

\section{APPENDIX}
\subsection{Logical rule encodings}
\label{appendix:logical_rule_encodings}
\inputminted[breaklines]{prolog}{"/home/jorrit/Data/KU Leuven/Semester 12/Capita Selecta H05N0a/deepproblog/src/deepproblog/examples/SORTOFCLEVR/model.pl"}


\bibliographystyle{bib_sobraep}
\bibliography{Capita_Selecta_AI_Initial_Idea} 

%\balance

\end{document}